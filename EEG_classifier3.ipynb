{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor Flow Version: 2.5.0\n",
      "Keras Version: 2.5.0\n",
      "\n",
      "Python 3.9.7 (default, Sep 16 2021, 23:53:23) \n",
      "[Clang 12.0.0 ]\n",
      "Pandas 1.3.5\n",
      "Scikit-Learn 1.0.2\n",
      "GPU is available\n"
     ]
    }
   ],
   "source": [
    "# What version of Python do you have?\n",
    "import sys\n",
    "import pandas as pd\n",
    "import sklearn as sk\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras\n",
    "\n",
    "print(f\"Tensor Flow Version: {tf.__version__}\")\n",
    "print(f\"Keras Version: {keras.__version__}\")\n",
    "print()\n",
    "print(f\"Python {sys.version}\")\n",
    "print(f\"Pandas {pd.__version__}\")\n",
    "print(f\"Scikit-Learn {sk.__version__}\")\n",
    "gpu = len(tf.config.list_physical_devices('GPU'))>0\n",
    "print(\"GPU is\", \"available\" if gpu else \"NOT AVAILABLE\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import layers\n",
    "import pyarrow.feather as feather\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "69aafc53-480f-4489-9b65-dd8bcd42fb70",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the training data\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/train_16/eyesclosed_train.feather'\n",
    "df_closed = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/train_16/eyesopen_train.feather'\n",
    "df_open = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/train_16/mathematic_train.feather'\n",
    "df_math = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/train_16/memory_train.feather'\n",
    "df_memory = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/train_16/music_train.feather'\n",
    "df_music = feather.read_feather(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7076, 6000) (7076, 6000) (7076, 6000) (7076, 6000) (7076, 6000)\n",
      "float16 float16 float16 float16 float16\n"
     ]
    }
   ],
   "source": [
    "# print shapes and data types \n",
    "print(df_closed.shape, df_open.shape, df_math.shape, df_memory.shape, df_music.shape)\n",
    "print(df_closed.dtypes[0], df_open.dtypes[0], df_math.dtypes[0], df_memory.dtypes[0], df_music.dtypes[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "24406b8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading in the validation data\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/val_16/eyesclosed_val.feather'\n",
    "df_closed_val = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/val_16/eyesopen_val.feather'\n",
    "df_open_val = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/val_16/mathematic_val.feather'\n",
    "df_math_val = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/val_16/memory_val.feather'\n",
    "df_memory_val = feather.read_feather(path)\n",
    "path = '/Users/asgnxt/mne-miniconda/mne_data/train_val_16/val_16/music_val.feather'\n",
    "df_music_val = feather.read_feather(path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # trimming data to 1/5th duration\n",
    "df_closed = df_closed.iloc[:,:int(df_closed.shape[1]/5)]\n",
    "df_open = df_open.iloc[:,:int(df_open.shape[1]/5)]\n",
    "df_math = df_math.iloc[:7076,:int(df_math.shape[1]/5)] #7076 rows to match other class data\n",
    "df_memory = df_memory.iloc[:7076,:int(df_memory.shape[1]/5)]\n",
    "df_music = df_music.iloc[:,:int(df_music.shape[1]/5)]\n",
    "\n",
    "df_closed_val = df_closed_val.iloc[:,:int(df_closed_val.shape[1]/5)]\n",
    "df_open_val = df_open_val.iloc[:,:int(df_open_val.shape[1]/5)]\n",
    "df_math_val = df_math_val.iloc[:7076,:int(df_math_val.shape[1]/5)]\n",
    "df_memory_val = df_memory_val.iloc[:7076,:int(df_memory_val.shape[1]/5)]\n",
    "df_music_val = df_music_val.iloc[:854,:int(df_music_val.shape[1]/5)] # 854 rows to match other class data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(854, 6000)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_music_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "33af3964",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "target_size = (61, 6000)\n",
      "Number of channels: 1\n",
      "Number of samples in closed: 116\n",
      "Number of samples in open: 116\n",
      "Number of samples in math: 116\n",
      "Number of samples in memory: 116\n",
      "Number of samples in music: 116\n",
      "Number of samples in closed_val: 14\n",
      "Number of samples in open_val: 14\n",
      "Number of samples in math_val: 14\n",
      "Number of samples in memory_val: 14\n",
      "Number of samples in music_val: 14\n",
      "Number of classes: 5\n"
     ]
    }
   ],
   "source": [
    "# defining parameters for the model\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "img_height = 61\n",
    "img_width = 6000\n",
    "target_size = (img_height, img_width)\n",
    "print(f'target_size = {target_size}')\n",
    "\n",
    "num_channels = 1\n",
    "print(f'Number of channels: {num_channels}')\n",
    "\n",
    "# calculating number of samples per class in training data\n",
    "num_samples_closed = int(df_closed.shape[0]/img_height)\n",
    "print(f'Number of samples in closed: {num_samples_closed}')\n",
    "num_samples_open = int(df_open.shape[0]/img_height)\n",
    "print(f'Number of samples in open: {num_samples_open}')\n",
    "num_samples_math = int(df_math.shape[0]/img_height)\n",
    "print(f'Number of samples in math: {num_samples_math}')\n",
    "num_samples_memory = int(df_memory.shape[0]/img_height)\n",
    "print(f'Number of samples in memory: {num_samples_memory}')\n",
    "num_samples_music = int(df_music.shape[0]/img_height)\n",
    "print(f'Number of samples in music: {num_samples_music}')\n",
    "\n",
    "# calculating number of samples per class in validation data\n",
    "num_samples_closed_val = int(df_closed_val.shape[0]/img_height)\n",
    "print(f'Number of samples in closed_val: {num_samples_closed_val}')\n",
    "num_samples_open_val = int(df_open_val.shape[0]/img_height)\n",
    "print(f'Number of samples in open_val: {num_samples_open_val}')\n",
    "num_samples_math_val = int(df_math_val.shape[0]/img_height)\n",
    "print(f'Number of samples in math_val: {num_samples_math_val}')\n",
    "num_samples_memory_val = int(df_memory_val.shape[0]/img_height)\n",
    "print(f'Number of samples in memory_val: {num_samples_memory_val}')\n",
    "num_samples_music_val = int(df_music_val.shape[0]/img_height)\n",
    "print(f'Number of samples in music_val: {num_samples_music_val}')\n",
    "\n",
    "# defining the number of classes\n",
    "class_names = [\"eyes_open\", \"eyes_closed\", \"math\", \"memory\", \"music\"]\n",
    "num_classes = len(class_names)\n",
    "print(f'Number of classes: {num_classes}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# converting dataframes to numpy arrays\n",
    "X_closed = pd.DataFrame.to_numpy(df_closed)\n",
    "X_open = pd.DataFrame.to_numpy(df_open)\n",
    "X_math = pd.DataFrame.to_numpy(df_math)\n",
    "X_memory = pd.DataFrame.to_numpy(df_memory)\n",
    "X_music = pd.DataFrame.to_numpy(df_music)\n",
    "X_closed_val = pd.DataFrame.to_numpy(df_closed_val)\n",
    "X_open_val = pd.DataFrame.to_numpy(df_open_val)\n",
    "X_math_val = pd.DataFrame.to_numpy(df_math_val)\n",
    "X_memory_val = pd.DataFrame.to_numpy(df_memory_val)\n",
    "X_music_val = pd.DataFrame.to_numpy(df_music_val)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reshaping the data\n",
    "X_closed = X_closed.reshape(num_samples_closed, img_height, img_width, num_channels)\n",
    "X_open = X_open.reshape(num_samples_open, img_height, img_width, num_channels)\n",
    "X_math = X_math.reshape(num_samples_math, img_height, img_width, num_channels)\n",
    "X_memory = X_memory.reshape(num_samples_memory, img_height, img_width, num_channels)\n",
    "X_music = X_music.reshape(num_samples_music, img_height, img_width, num_channels)\n",
    "X_closed_val = X_closed_val.reshape(num_samples_closed_val, img_height, img_width, num_channels)\n",
    "X_open_val = X_open_val.reshape(num_samples_open_val, img_height, img_width, num_channels)\n",
    "X_math_val = X_math_val.reshape(num_samples_math_val, img_height, img_width, num_channels)\n",
    "X_memory_val = X_memory_val.reshape(num_samples_memory_val, img_height, img_width, num_channels)\n",
    "X_music_val = X_music_val.reshape(num_samples_music_val, img_height, img_width, num_channels)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the data\n",
    "X_tr = np.vstack((X_closed, X_open, X_math, X_memory, X_music))\n",
    "X_val = np.vstack((X_closed_val, X_open_val, X_math_val, X_memory_val, X_music_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training data dimensions: (580, 61, 6000, 1)\n",
      "validation data dimensions: (70, 61, 6000, 1)\n"
     ]
    }
   ],
   "source": [
    "print(f'training data dimensions: {X_tr.shape}')\n",
    "print(f'validation data dimensions: {X_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# labels for the data\n",
    "labels_closed = np.zeros(num_samples_closed)\n",
    "labels_open = np.ones(num_samples_open)\n",
    "labels_math = np.full(num_samples_math, 2)\n",
    "labels_memory = np.full(num_samples_memory, 3)\n",
    "labels_music = np.full(num_samples_music, 4)\n",
    "labels_closed_val = np.zeros(num_samples_closed_val)\n",
    "labels_open_val = np.ones(num_samples_open_val)\n",
    "labels_math_val = np.full(num_samples_math_val, 2)\n",
    "labels_memory_val = np.full(num_samples_memory_val, 3)\n",
    "labels_music_val = np.full(num_samples_music_val, 4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stacking the labels\n",
    "y_tr = np.hstack((labels_closed, labels_open, labels_math, labels_memory, labels_music))\n",
    "y_val = np.hstack((labels_closed_val, labels_open_val, labels_math_val, labels_memory_val, labels_music_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# convert class vectors to binary class matrices\n",
    "y_tr = keras.utils.to_categorical(y_tr, num_classes)\n",
    "y_val = keras.utils.to_categorical(y_val, num_classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "training labels dimensions: (580, 5)\n",
      "validation labels dimensions: (70, 5)\n"
     ]
    }
   ],
   "source": [
    "print(f'training labels dimensions: {y_tr.shape}')\n",
    "print(f'validation labels dimensions: {y_val.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = (61, 6000, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Please add `keras.layers.InputLayer` instead of `keras.Input` to Sequential model. `keras.Input` is intended to be used by Functional model.\n",
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "module_wrapper_7 (ModuleWrap (None, 59, 5998, 32)      320       \n",
      "_________________________________________________________________\n",
      "module_wrapper_8 (ModuleWrap (None, 29, 2999, 32)      0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_9 (ModuleWrap (None, 27, 2997, 64)      18496     \n",
      "_________________________________________________________________\n",
      "module_wrapper_10 (ModuleWra (None, 13, 1498, 64)      0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_11 (ModuleWra (None, 1246336)           0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_12 (ModuleWra (None, 1246336)           0         \n",
      "_________________________________________________________________\n",
      "module_wrapper_13 (ModuleWra (None, 5)                 6231685   \n",
      "=================================================================\n",
      "Total params: 6,250,501\n",
      "Trainable params: 6,250,501\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = keras.Sequential(\n",
    "    [\n",
    "        keras.Input(shape=input_shape),\n",
    "        layers.Conv2D(32, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
    "        layers.MaxPooling2D(pool_size=(2, 2)),\n",
    "        layers.Flatten(),\n",
    "        layers.Dropout(0.5),\n",
    "        layers.Dense(num_classes, activation=\"softmax\"),\n",
    "    ]\n",
    ")\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 08:36:54.920280: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:176] None of the MLIR Optimization Passes are enabled (registered 2)\n",
      "2022-03-27 08:36:54.922943: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n",
      "2022-03-27 08:36:55.050352: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/19\n",
      "19/19 [==============================] - ETA: 0s - loss: 1.6212 - accuracy: 0.1983"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-27 08:37:18.086362: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:112] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "19/19 [==============================] - 24s 1s/step - loss: 1.6212 - accuracy: 0.1983 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 2/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 3/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 4/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 5/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 6/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 7/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 8/19\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 9/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6096 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 10/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 11/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 12/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 13/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6095 - val_accuracy: 0.2000\n",
      "Epoch 14/19\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 15/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 16/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 17/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6094 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 18/19\n",
      "19/19 [==============================] - 22s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n",
      "Epoch 19/19\n",
      "19/19 [==============================] - 21s 1s/step - loss: 1.6095 - accuracy: 0.2000 - val_loss: 1.6094 - val_accuracy: 0.2000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x280a9a730>"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train the model\n",
    "epochs = 19\n",
    "\n",
    "model.compile(loss=\"categorical_crossentropy\", optimizer=\"adam\", metrics=[\"accuracy\"])\n",
    "\n",
    "model.fit(X_tr, y_tr, batch_size=batch_size, epochs=epochs, validation_data=(X_val, y_val), verbose=1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "b541e3e183694e8fa6a526cacf085d58b01826ae92a5e7e7ec9cf91fd02c76b0"
  },
  "kernelspec": {
   "display_name": "Python 3.9 (tensorflow)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
